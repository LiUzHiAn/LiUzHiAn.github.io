<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="一个记录关于生活,计算机,成长,旅行的地方">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
          从源码学习 Faster-RCNN - 刘知安的博客 | LiuZhian&#39;s Blog
        
    </title>

    <link rel="canonical" href="https://liuzhian.github.io/2020/12/02/从源码学习 Faster-RCNN/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/hux-blog.min.css">


    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">LiuZhian&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">主页</a>
                    </li>

                    

                        
                    

                        
                    

                        
                        <li>
                            <a href="/about/">关于我</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archives/">随笔</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">归类</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    

<!-- Image to hack wechat -->
<!-- <img src="https://liuzhian.github.io/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="{{ site.baseurl }}/{% if page.header-img %}{{ page.header-img }}{% else %}{{ site.header-img }}{% endif %}" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/posts_imgs/hello-world.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                          <a class="tag" href="/tags/#Object Detection" title="Object Detection">Object Detection</a>
                        
                    </div>
                    <h1>从源码学习 Faster-RCNN</h1>
                    <h2 class="subheading">Learn from source code</h2>
                    <span class="meta">
                        Posted by 刘知安 on
                        2020-12-02
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">
				
				<!-- 文章目录全局默认开启，如果不加目录，在文章front-matter设置toc为false -->  
				
				<div id="toc" class="toc-article">
				
                    <strong class="toc-title">文章目录</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#从源码学习-Faster-RCNN"><span class="toc-text">从源码学习 Faster-RCNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-GenerializedRCNN类"><span class="toc-text">1. GenerializedRCNN类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-FasterRCNN类"><span class="toc-text">2. FasterRCNN类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-backbone"><span class="toc-text">2.1 backbone</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-transform"><span class="toc-text">2.2 transform</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-RPN"><span class="toc-text">2.3 RPN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-ROI-head"><span class="toc-text">2.4 ROI_head</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-Box-head"><span class="toc-text">2.5 Box head</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6-最后的筛选"><span class="toc-text">2.6 最后的筛选</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-7-transform逆变换"><span class="toc-text">2.7 transform逆变换</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-最后的最后"><span class="toc-text">3. 最后的最后</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Demo"><span class="toc-text">4. Demo</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献"><span class="toc-text">参考文献</span></a></li></ol></li></ol>
                    
                    <!-- 文章目录用的是themes\huxblog\source\css下的post.style,hexo会把它生成一个post.css文件在public文件夹中 -->
                    
<link rel="stylesheet" href="/css/post.css">

				
                </div>
				
				
                <h1 id="从源码学习-Faster-RCNN"><a href="#从源码学习-Faster-RCNN" class="headerlink" title="从源码学习 Faster-RCNN"></a>从源码学习 Faster-RCNN</h1><p><strong>说在前面的话</strong>：</p>
<p>一直认为光读论文没什么用，因为很多实现的trick是很难用文字可以表达的，结合代码和论文往往可以更好地get到作者的意思。在看本文之前，强烈建议阅读<a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">[1]</a>，对Faster-RCNN的大致处理流程有个感性的认识。此外，声明一点，在本文中，RoI(Region of Interest)和proposal你可以认为就是一个东西。</p>
<p>Faster-RCNN已经被收录到了Pytorch官方实现中，在 <code>torchvision.models.detection.faster_rcnn</code>中。</p>
<h3 id="1-GenerializedRCNN类"><a href="#1-GenerializedRCNN类" class="headerlink" title="1. GenerializedRCNN类"></a>1. GenerializedRCNN类</h3><p><code>FasterRCNN</code>类的继承关系为 <code>FasterRCNN --&gt; GenerializedRCNN --&gt; nn.Module</code>，我们先看一下<code>GenerializedRCNN</code>类的实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GeneralizedRCNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Main class for Generalized R-CNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        backbone (nn.Module):</span></span><br><span class="line"><span class="string">        rpn (nn.Module):</span></span><br><span class="line"><span class="string">        heads (nn.Module): takes the features + the proposals from the RPN and computes</span></span><br><span class="line"><span class="string">            detections / masks from it.</span></span><br><span class="line"><span class="string">        transform (nn.Module): performs the data transformation from the inputs to feed into</span></span><br><span class="line"><span class="string">            the model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, backbone, rpn, roi_heads, transform)</span>:</span></span><br><span class="line">        super(GeneralizedRCNN, self).__init__()</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.backbone = backbone</span><br><span class="line">        self.rpn = rpn</span><br><span class="line">        self.roi_heads = roi_heads</span><br><span class="line">        <span class="comment"># used only on torchscript mode</span></span><br><span class="line">        self._has_warned = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>正如文档所说，<code>GenerializedRCNN</code>类就是一个对Faster-RCNN模型的抽象，主要包含3大部分：backbone，RPN和roi_heads，三者的功能分别是提取特征，得到region proposals，对region proposals中的物体进行分类和bbox regression，具体的细节可以参阅<a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">[1]</a>。至于这里的transformer，就是用于输入backbone之前的数据处理的。</p>
<p>再看下GenerializedRCNN类的<code>forawrd()</code>函数，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, images, targets=None)</span>:</span></span><br><span class="line">    <span class="comment"># type: (List[Tensor], Optional[List[Dict[str, Tensor]]])</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        images (list[Tensor]): images to be processed</span></span><br><span class="line"><span class="string">        targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        result (list[BoxList] or dict[Tensor]): the output from the model.</span></span><br><span class="line"><span class="string">            During training, it returns a dict[Tensor] which contains the losses.</span></span><br><span class="line"><span class="string">            During testing, it returns list[BoxList] contains additional fields</span></span><br><span class="line"><span class="string">            like `scores`, `labels` and `mask` (for Mask R-CNN models).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> self.training <span class="keyword">and</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"In training mode, targets should be passed"</span>)</span><br><span class="line">    original_image_sizes = torch.jit.annotate(List[Tuple[int, int]], [])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># NOTE 1: 保留图片原始大小</span></span><br><span class="line">    <span class="keyword">for</span> img <span class="keyword">in</span> images:</span><br><span class="line">        val = img.shape[<span class="number">-2</span>:]</span><br><span class="line">        <span class="keyword">assert</span> len(val) == <span class="number">2</span></span><br><span class="line">        original_image_sizes.append((val[<span class="number">0</span>], val[<span class="number">1</span>]))</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># NOTE 2: 数据预处理</span></span><br><span class="line">    images, targets = self.transform(images, targets)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># NOTE 3: 得到featire map</span></span><br><span class="line">    features = self.backbone(images.tensors)</span><br><span class="line">    <span class="keyword">if</span> isinstance(features, torch.Tensor):</span><br><span class="line">        features = OrderedDict([(<span class="string">'0'</span>, features)])</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># NOTE 4: 得到region proposals</span></span><br><span class="line">    proposals, proposal_losses = self.rpn(images, features, targets)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># NOTE 5: 对region proposals进行分类和bbox regression</span></span><br><span class="line">    detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># NOTE 6: 数据后处理</span></span><br><span class="line">    detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)</span><br><span class="line"></span><br><span class="line">    losses = &#123;&#125;</span><br><span class="line">    losses.update(detector_losses)</span><br><span class="line">    losses.update(proposal_losses)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> torch.jit.is_scripting():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._has_warned:</span><br><span class="line">            warnings.warn(<span class="string">"RCNN always returns a (Losses, Detections) tuple in scripting"</span>)</span><br><span class="line">            self._has_warned = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> (losses, detections)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.eager_outputs(losses, detections)</span><br></pre></td></tr></table></figure>
<p>根据代码，我们可以很清晰的看到大致的处理流程，也就是代码中注释的NOET 1-6，由于2-6步都是比较general的步骤，这里讲一下数据预处理的步骤：</p>
<ul>
<li>NOTE-1 数据预处理</li>
</ul>
<p>对图片的数据处理倒很常见，首先缩放倒固定的大小，然后归一化到$[0,1]$之间，再减去ImageNet的均值，除以方差。而在训练阶段，对bbox的ground truth，由于网络最终实际上是在固定大小的图片上进行训练的，因此需要对原始的bbox gt也进行相应的缩放。而在实际test/inference阶段，我们肯定希望最终结果的bbox是基于原始输入图像（即缩放前的图片）而言的，因此在数据预处理时，我们要记录缩放的一些信息。</p>
<ul>
<li>NOTE-6 数据后处理</li>
</ul>
<p>在得到了detections之后，我们需要按照预处理时存储的图片原始信息进行缩放变换。</p>
<p>数据预处理和后处理的整体流程如下图所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-8f447833ee49a5f0a62e60a560afc5a3_1440w.jpg" alt="img"></p>
<p>而中间的三大组建处理流程则如下图所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-f66c2b8887240322ed0497b9aee71857_1440w.jpg" alt="img"></p>
<h3 id="2-FasterRCNN类"><a href="#2-FasterRCNN类" class="headerlink" title="2. FasterRCNN类"></a>2. FasterRCNN类</h3><p> 为了更好的说明问题，以及方便表达，下文中统一使用下面这个例子来debug</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models.detection.faster_rcnn <span class="keyword">import</span> fasterrcnn_resnet50_fpn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = fasterrcnn_resnet50_fpn()</span><br><span class="line">model.eval()</span><br><span class="line">x = [torch.rand(<span class="number">3</span>, <span class="number">300</span>, <span class="number">400</span>), torch.rand(<span class="number">3</span>, <span class="number">500</span>, <span class="number">400</span>)]</span><br><span class="line">predictions = model(x)</span><br><span class="line"></span><br><span class="line">print(predictions)</span><br></pre></td></tr></table></figure>
<p>Let‘s get started!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FasterRCNN</span><span class="params">(GeneralizedRCNN)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements Faster R-CNN.</span></span><br><span class="line"><span class="string">    ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, backbone, num_classes=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 # transform parameters</span></span></span><br><span class="line"><span class="function"><span class="params">                 min_size=<span class="number">800</span>, max_size=<span class="number">1333</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 image_mean=None, image_std=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 # RPN parameters</span></span></span><br><span class="line"><span class="function"><span class="params">                 rpn_anchor_generator=None, rpn_head=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 rpn_pre_nms_top_n_train=<span class="number">2000</span>, rpn_pre_nms_top_n_test=<span class="number">1000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 rpn_post_nms_top_n_train=<span class="number">2000</span>, rpn_post_nms_top_n_test=<span class="number">1000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 rpn_nms_thresh=<span class="number">0.7</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 rpn_fg_iou_thresh=<span class="number">0.7</span>, rpn_bg_iou_thresh=<span class="number">0.3</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 rpn_batch_size_per_image=<span class="number">256</span>, rpn_positive_fraction=<span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 # Box parameters</span></span></span><br><span class="line"><span class="function"><span class="params">                 box_roi_pool=None, box_head=None, box_predictor=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 box_score_thresh=<span class="number">0.05</span>, box_nms_thresh=<span class="number">0.5</span>, box_detections_per_img=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 box_fg_iou_thresh=<span class="number">0.5</span>, box_bg_iou_thresh=<span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 box_batch_size_per_image=<span class="number">512</span>, box_positive_fraction=<span class="number">0.25</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bbox_reg_weights=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(backbone, <span class="string">"out_channels"</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">"backbone should contain an attribute out_channels "</span></span><br><span class="line">                <span class="string">"specifying the number of output channels (assumed to be the "</span></span><br><span class="line">                <span class="string">"same for all the levels)"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> isinstance(rpn_anchor_generator, (AnchorGenerator, type(<span class="literal">None</span>)))</span><br><span class="line">        <span class="keyword">assert</span> isinstance(box_roi_pool, (MultiScaleRoIAlign, type(<span class="literal">None</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> box_predictor <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">"num_classes should be None when box_predictor is specified"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> box_predictor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">"num_classes should not be None when box_predictor "</span></span><br><span class="line">                                 <span class="string">"is not specified"</span>)</span><br><span class="line"></span><br><span class="line">        out_channels = backbone.out_channels</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rpn_anchor_generator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            anchor_sizes = ((<span class="number">32</span>,), (<span class="number">64</span>,), (<span class="number">128</span>,), (<span class="number">256</span>,), (<span class="number">512</span>,))</span><br><span class="line">            aspect_ratios = ((<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>),) * len(anchor_sizes)</span><br><span class="line">            rpn_anchor_generator = AnchorGenerator(</span><br><span class="line">                anchor_sizes, aspect_ratios</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">if</span> rpn_head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            rpn_head = RPNHead(</span><br><span class="line">                out_channels, rpn_anchor_generator.num_anchors_per_location()[<span class="number">0</span>]</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)</span><br><span class="line">        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)</span><br><span class="line"></span><br><span class="line">        rpn = RegionProposalNetwork(</span><br><span class="line">            rpn_anchor_generator, rpn_head,</span><br><span class="line">            rpn_fg_iou_thresh, rpn_bg_iou_thresh,</span><br><span class="line">            rpn_batch_size_per_image, rpn_positive_fraction,</span><br><span class="line">            rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> box_roi_pool <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            box_roi_pool = MultiScaleRoIAlign(</span><br><span class="line">                featmap_names=[<span class="string">'0'</span>, <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>],</span><br><span class="line">                output_size=<span class="number">7</span>,</span><br><span class="line">                sampling_ratio=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> box_head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            resolution = box_roi_pool.output_size[<span class="number">0</span>]</span><br><span class="line">            representation_size = <span class="number">1024</span></span><br><span class="line">            box_head = TwoMLPHead(</span><br><span class="line">                out_channels * resolution ** <span class="number">2</span>,</span><br><span class="line">                representation_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> box_predictor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            representation_size = <span class="number">1024</span></span><br><span class="line">            box_predictor = FastRCNNPredictor(</span><br><span class="line">                representation_size,</span><br><span class="line">                num_classes)</span><br><span class="line"></span><br><span class="line">        roi_heads = RoIHeads(</span><br><span class="line">            <span class="comment"># Box</span></span><br><span class="line">            box_roi_pool, box_head, box_predictor,</span><br><span class="line">            box_fg_iou_thresh, box_bg_iou_thresh,</span><br><span class="line">            box_batch_size_per_image, box_positive_fraction,</span><br><span class="line">            bbox_reg_weights,</span><br><span class="line">            box_score_thresh, box_nms_thresh, box_detections_per_img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> image_mean <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            image_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">        <span class="keyword">if</span> image_std <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            image_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)</span><br><span class="line"></span><br><span class="line">        super(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)</span><br></pre></td></tr></table></figure>
<p>通过代码<code>super(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)</code>可以看到，<code>FasterRCNN</code>类继承了<code>GeneralizedRCNNTransform</code>类。那这4个组件分别是什么呢？我们分别来看下。</p>
<h4 id="2-1-backbone"><a href="#2-1-backbone" class="headerlink" title="2.1 backbone"></a>2.1 backbone</h4><p>这东西就不用多讲了，backbone网络，可以是VGG、ResNet或其他任何基础网络结构。</p>
<h4 id="2-2-transform"><a href="#2-2-transform" class="headerlink" title="2.2 transform"></a>2.2 transform</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> image_mean <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">		image_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line"><span class="keyword">if</span> image_std <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">	  image_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)</span><br></pre></td></tr></table></figure>
<p>从这几行代码，熟悉的朋友肯定知道这几个数字的含义，其实就是ImageNet的均值和方差，所以，这里在做的工作就是mean-std 归一化和图片大小的缩放。</p>
<h4 id="2-3-RPN"><a href="#2-3-RPN" class="headerlink" title="2.3 RPN"></a>2.3 RPN</h4><p>RPN是Faster-RCNN中的重头戏。从代码中可以看出，将RPN分成了两块，即<code>rpn_anchor_generator</code>和<code>rpn_head</code>，我们先看前者。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> rpn_anchor_generator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    anchor_sizes = ((<span class="number">32</span>,), (<span class="number">64</span>,), (<span class="number">128</span>,), (<span class="number">256</span>,), (<span class="number">512</span>,))</span><br><span class="line">    aspect_ratios = ((<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>),) * len(anchor_sizes)</span><br><span class="line">    rpn_anchor_generator = AnchorGenerator(</span><br><span class="line">        anchor_sizes, aspect_ratios</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>可以看到，对于每个anchor size，都会对应有3种aspect ratio，并构造出一个<code>AnchorGenerator</code>对象。细心的你可能会发现，为啥<code>anchor_size</code>和<code>aspect_ratios</code>都是二维的tuple？按照Faster-RCNN中的理论，RPN接受一个feature map，然后按照不同size和不同aspect ratio组合，得到很多不同尺寸的anchor，两个一维的tuple就够了呀？记住，这里是为了更generally地集成其他backbone，例如多尺度的FPN，分别用不同尺度的特征来负责不同尺度大小的anchor，所以二维元组中的每一个代表是一个尺度。</p>
<p>OK，上面这个trick知道了以后，<code>AnchorGenerator</code>又是怎么实现的呢？别急，慢慢看</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnchorGenerator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Module that generates anchors for a set of feature maps and</span></span><br><span class="line"><span class="string">    image sizes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The module support computing anchors at multiple sizes and aspect ratios</span></span><br><span class="line"><span class="string">    per feature map.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    sizes and aspect_ratios should have the same number of elements, and it should</span></span><br><span class="line"><span class="string">    correspond to the number of feature maps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    sizes[i] and aspect_ratios[i] can have an arbitrary number of elements,</span></span><br><span class="line"><span class="string">    and AnchorGenerator will output a set of sizes[i] * aspect_ratios[i] anchors</span></span><br><span class="line"><span class="string">    per spatial location for feature map i.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        sizes (Tuple[Tuple[int]]):</span></span><br><span class="line"><span class="string">        aspect_ratios (Tuple[Tuple[float]]):</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        sizes=<span class="params">(<span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>)</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        aspect_ratios=<span class="params">(<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>)</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        super(AnchorGenerator, self).__init__()</span><br><span class="line">				<span class="comment"># 如果输入的是1维tuple，那转为2维，对应多尺度特征</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(sizes[<span class="number">0</span>], (list, tuple)):</span><br><span class="line">            <span class="comment"># TODO change this</span></span><br><span class="line">            sizes = tuple((s,) <span class="keyword">for</span> s <span class="keyword">in</span> sizes)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(aspect_ratios[<span class="number">0</span>], (list, tuple)):</span><br><span class="line">            aspect_ratios = (aspect_ratios,) * len(sizes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> len(sizes) == len(aspect_ratios)</span><br><span class="line"></span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.aspect_ratios = aspect_ratios</span><br><span class="line">        self.cell_anchors = <span class="literal">None</span></span><br><span class="line">        self._cache = &#123;&#125;</span><br><span class="line"></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">generate_anchors</span><span class="params">(self, scales, aspect_ratios, dtype=torch.float32, device=<span class="string">"cpu"</span>)</span>:</span></span><br><span class="line">        <span class="comment"># type: (List[int], List[float], int, Device)  # noqa: F821</span></span><br><span class="line">        scales = torch.as_tensor(scales, dtype=dtype, device=device)</span><br><span class="line">        aspect_ratios = torch.as_tensor(aspect_ratios, dtype=dtype, device=device)</span><br><span class="line">        h_ratios = torch.sqrt(aspect_ratios)</span><br><span class="line">        w_ratios = <span class="number">1</span> / h_ratios</span><br><span class="line"></span><br><span class="line">        ws = (w_ratios[:, <span class="literal">None</span>] * scales[<span class="literal">None</span>, :]).view(<span class="number">-1</span>)</span><br><span class="line">        hs = (h_ratios[:, <span class="literal">None</span>] * scales[<span class="literal">None</span>, :]).view(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=<span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> base_anchors.round()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_cell_anchors</span><span class="params">(self, dtype, device)</span>:</span></span><br><span class="line">        <span class="comment"># type: (int, Device) -&gt; None    # noqa: F821</span></span><br><span class="line">        <span class="keyword">if</span> self.cell_anchors <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            cell_anchors = self.cell_anchors</span><br><span class="line">            <span class="keyword">assert</span> cell_anchors <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            <span class="comment"># suppose that all anchors have the same device</span></span><br><span class="line">            <span class="comment"># which is a valid assumption in the current state of the codebase</span></span><br><span class="line">            <span class="keyword">if</span> cell_anchors[<span class="number">0</span>].device == device:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">				</span><br><span class="line">        <span class="comment"># 生成feature map上每个cell处的anchors</span></span><br><span class="line">        cell_anchors = [</span><br><span class="line">            self.generate_anchors(</span><br><span class="line">                sizes,</span><br><span class="line">                aspect_ratios,</span><br><span class="line">                dtype,</span><br><span class="line">                device</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> sizes, aspect_ratios <span class="keyword">in</span> zip(self.sizes, self.aspect_ratios)</span><br><span class="line">        ]</span><br><span class="line">        self.cell_anchors = cell_anchors</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_anchors_per_location</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [len(s) * len(a) <span class="keyword">for</span> s, a <span class="keyword">in</span> zip(self.sizes, self.aspect_ratios)]</span><br></pre></td></tr></table></figure>
<p>我们从Faster-RCNN的<code>rpn_anchor_generator</code>可以看出，用的是5种尺度(32, 64, 128, 256, 512)和3种aspect ratio(0.5, 1.0, 2.0)，组合起来就是15种anchors。也就是下面这15个组合，注意，anchor是以(0,0)为基准的，并且得到的anchors是在feature map上每个cell处的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[ -23.,  -11.,   23.,   11.]</span><br><span class="line">[ -16.,  -16.,   16.,   16.] # scale &#x3D; 32</span><br><span class="line">[ -11.,  -23.,   11.,   23.] </span><br><span class="line">[ -45.,  -23.,   45.,   23.]</span><br><span class="line">[ -32.,  -32.,   32.,   32.] # scale &#x3D; 64</span><br><span class="line">[ -23.,  -45.,   23.,   45.]</span><br><span class="line">[ -91.,  -45.,   91.,   45.]</span><br><span class="line">[ -64.,  -64.,   64.,   64.] # scale &#x3D; 128</span><br><span class="line">[ -45.,  -91.,   45.,   91.]</span><br><span class="line">[-181.,  -91.,  181.,   91.]</span><br><span class="line">[-128., -128.,  128.,  128.] # scale &#x3D; 256</span><br><span class="line">[ -91., -181.,   91.,  181.]</span><br><span class="line">[-362., -181.,  362.,  181.]</span><br><span class="line">[-256., -256.,  256.,  256.] # scale &#x3D; 512</span><br><span class="line">[-181., -362.,  181.,  362.]</span><br></pre></td></tr></table></figure>
<p>在得到了每个cell（也就是feature map上每个位置处）的anchors大小后，我们怎么得到整个图片上的anchors呢？根据CNN中卷积的特点，我们可以知道feature map上的每个cell其实是有一定程度的感受野大小的，也feature map size关于原始输入图片的缩放倍数，在RPN里面，这个变量被叫做stride，计算方法为$\text{stride}=\frac{\text{feature map size}}{\text{img size}}$。</p>
<p>好，我们来看下代码是怎么实现的。主要关注<code>grid_anchors()</code>函数就OK了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For every combination of (a, (g, s), i) in (self.cell_anchors, zip(grid_sizes, strides), 0:2),</span></span><br><span class="line"><span class="comment"># output g[i] anchors that are s[i] distance apart in direction i, with the same dimensions as a.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grid_anchors</span><span class="params">(self, grid_sizes, strides)</span>:</span></span><br><span class="line">    <span class="comment"># type: (List[List[int]], List[List[Tensor]])</span></span><br><span class="line">    anchors = []</span><br><span class="line">    cell_anchors = self.cell_anchors</span><br><span class="line">    <span class="keyword">assert</span> cell_anchors <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> size, stride, base_anchors <span class="keyword">in</span> zip(</span><br><span class="line">        grid_sizes, strides, cell_anchors</span><br><span class="line">    ):</span><br><span class="line">        grid_height, grid_width = size</span><br><span class="line">        stride_height, stride_width = stride</span><br><span class="line">        device = base_anchors.device</span><br><span class="line">				</span><br><span class="line">        <span class="comment"># For output anchor, compute [x_center, y_center, x_center, y_center]</span></span><br><span class="line">        <span class="comment"># NOTE 生成每个网格个点的偏移量</span></span><br><span class="line">        shifts_x = torch.arange(</span><br><span class="line">            <span class="number">0</span>, grid_width, dtype=torch.float32, device=device</span><br><span class="line">        ) * stride_width</span><br><span class="line">        shifts_y = torch.arange(</span><br><span class="line">            <span class="number">0</span>, grid_height, dtype=torch.float32, device=device</span><br><span class="line">        ) * stride_height</span><br><span class="line">        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)</span><br><span class="line">        shift_x = shift_x.reshape(<span class="number">-1</span>)</span><br><span class="line">        shift_y = shift_y.reshape(<span class="number">-1</span>)</span><br><span class="line">        shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For every (base anchor, output anchor) pair,</span></span><br><span class="line">        <span class="comment"># offset each zero-centered base anchor by the center of the output anchor.</span></span><br><span class="line">        anchors.append(</span><br><span class="line">            (shifts.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">4</span>) + base_anchors.view(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">4</span>)).reshape(<span class="number">-1</span>, <span class="number">4</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> anchors</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cached_grid_anchors</span><span class="params">(self, grid_sizes, strides)</span>:</span></span><br><span class="line">    <span class="comment"># type: (List[List[int]], List[List[Tensor]])</span></span><br><span class="line">    key = str(grid_sizes) + str(strides)</span><br><span class="line">    <span class="keyword">if</span> key <span class="keyword">in</span> self._cache:</span><br><span class="line">        <span class="keyword">return</span> self._cache[key]</span><br><span class="line">    anchors = self.grid_anchors(grid_sizes, strides)</span><br><span class="line">    self._cache[key] = anchors</span><br><span class="line">    <span class="keyword">return</span> anchors</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, image_list, feature_maps)</span>:</span></span><br><span class="line">    <span class="comment"># type: (ImageList, List[Tensor])</span></span><br><span class="line">    grid_sizes = list([feature_map.shape[<span class="number">-2</span>:] <span class="keyword">for</span> feature_map <span class="keyword">in</span> feature_maps])</span><br><span class="line">    image_size = image_list.tensors.shape[<span class="number">-2</span>:]</span><br><span class="line">    dtype, device = feature_maps[<span class="number">0</span>].dtype, feature_maps[<span class="number">0</span>].device</span><br><span class="line">    strides = [[torch.tensor(image_size[<span class="number">0</span>] / g[<span class="number">0</span>], dtype=torch.int64, device=device),</span><br><span class="line">                torch.tensor(image_size[<span class="number">1</span>] / g[<span class="number">1</span>], dtype=torch.int64, device=device)] <span class="keyword">for</span> g <span class="keyword">in</span> grid_sizes]</span><br><span class="line">    self.set_cell_anchors(dtype, device)</span><br><span class="line">    anchors_over_all_feature_maps = self.cached_grid_anchors(grid_sizes, strides)</span><br><span class="line">    anchors = torch.jit.annotate(List[List[torch.Tensor]], [])</span><br><span class="line">    <span class="keyword">for</span> i, (image_height, image_width) <span class="keyword">in</span> enumerate(image_list.image_sizes):</span><br><span class="line">        anchors_in_image = []</span><br><span class="line">        <span class="keyword">for</span> anchors_per_feature_map <span class="keyword">in</span> anchors_over_all_feature_maps:</span><br><span class="line">            anchors_in_image.append(anchors_per_feature_map)</span><br><span class="line">        anchors.append(anchors_in_image)</span><br><span class="line">    anchors = [torch.cat(anchors_per_image) <span class="keyword">for</span> anchors_per_image <span class="keyword">in</span> anchors]</span><br><span class="line">    <span class="comment"># Clear the cache in case that memory leaks.</span></span><br><span class="line">    self._cache.clear()</span><br><span class="line">    <span class="keyword">return</span> anchors</span><br></pre></td></tr></table></figure>
<p>从<code>grid_anchors()</code>函数中可以看出，会先根据stride和feature map的大小（也就是grid_weight和grid_height）生成一个网格，网格上第$(i,j)$个元素代表的是第$(i,j)$位置处的anchors相对于base_anchors的偏移量。</p>
<p><img src="https://pic1.zhimg.com/80/v2-4d7856217741e923a50ac4157c8da324_1440w.jpg" alt="img"></p>
<p>为啥要偏移量这个概念呢？别忘了，我们在上一步每个cell处生成anchors的时候，中心是(0,0)。如果加上现在的偏移量，即通过<code>anchors.append((shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4))</code>这条语句，就是每个cell真正的anchor位置了！一图胜千语，看下面这个图你肯定就明白了。</p>
<p><img src="https://pic2.zhimg.com/80/v2-793e1219b408d39df9fababa776e0899_1440w.jpg" alt="img"></p>
<p>现在所有的anchors都得到了，接下来就要通过几层网络来判断每个 anchor 是否有目标，同时还需要回归有 bounding box 需要的4个值 $(dx,dy,dw,dh)$。也就是<code>rpn_head</code>了，相比<code>rpn_anchor_generator</code>，还是很简单的，其实也就是个<code>RPNHead</code>类对象，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RPNHead</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Adds a simple RPN Head with classification and regression heads</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        in_channels (int): number of channels of the input feature</span></span><br><span class="line"><span class="string">        num_anchors (int): number of anchors to be predicted</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, num_anchors)</span>:</span></span><br><span class="line">        super(RPNHead, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(</span><br><span class="line">            in_channels, in_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.bbox_pred = nn.Conv2d(</span><br><span class="line">            in_channels, num_anchors * <span class="number">4</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> self.children():</span><br><span class="line">            torch.nn.init.normal_(l.weight, std=<span class="number">0.01</span>)</span><br><span class="line">            torch.nn.init.constant_(l.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># type: (List[Tensor])</span></span><br><span class="line">        logits = []</span><br><span class="line">        bbox_reg = []</span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> x:</span><br><span class="line">            t = F.relu(self.conv(feature))</span><br><span class="line">            logits.append(self.cls_logits(t))</span><br><span class="line">            bbox_reg.append(self.bbox_pred(t))</span><br><span class="line">        <span class="keyword">return</span> logits, bbox_reg</span><br></pre></td></tr></table></figure>
<p>其实无非就是几层全连接和激活函数。需要注意一下的是，RPN_head的输入通道数就是backbone网络得到的feature map的通道数，在Faster-RCNN代码中，我们也可以很明显地确认到这一点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out_channels = backbone.out_channels</span><br><span class="line"></span><br><span class="line">rpn_head = RPNHead(</span><br><span class="line">		out_channels, rpn_anchor_generator.num_anchors_per_location()[<span class="number">0</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>那RPN_head的输入通道数呢？显然有两部分，第一部分是判断每个anchor里面是否有物体的logits，第二部分则是每个anchor的4个回归参数 $(dx,dy,dw,dh)$。如下图所示，这里仅仅展示的是一个feature map的情况，多尺度的case类似处理。</p>
<p><img src="https://pic3.zhimg.com/80/v2-59bea2f6fb4c316ac413c81fe14e4ed6_1440w.jpg" alt="img"></p>
<p>呼，rpn的两个部分都处理好了，终于可以组装起来得到一个RPN对象，并训练这个RPN网络了。我们看下Faster-RCNN中关于RPN的定义和对应的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义</span></span><br><span class="line">rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)</span><br><span class="line">rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)</span><br><span class="line"></span><br><span class="line">rpn = RegionProposalNetwork(</span><br><span class="line">    rpn_anchor_generator, rpn_head,</span><br><span class="line">    rpn_fg_iou_thresh, rpn_bg_iou_thresh,</span><br><span class="line">    rpn_batch_size_per_image, rpn_positive_fraction,</span><br><span class="line">    rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RegionProposalNetwork</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements Region Proposal Network (RPN).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature</span></span><br><span class="line"><span class="string">            maps.</span></span><br><span class="line"><span class="string">        head (nn.Module): module that computes the objectness and regression deltas</span></span><br><span class="line"><span class="string">        fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be</span></span><br><span class="line"><span class="string">            considered as positive during training of the RPN.</span></span><br><span class="line"><span class="string">        bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be</span></span><br><span class="line"><span class="string">            considered as negative during training of the RPN.</span></span><br><span class="line"><span class="string">        batch_size_per_image (int): number of anchors that are sampled during training of the RPN</span></span><br><span class="line"><span class="string">            for computing the loss</span></span><br><span class="line"><span class="string">        positive_fraction (float): proportion of positive anchors in a mini-batch during training</span></span><br><span class="line"><span class="string">            of the RPN</span></span><br><span class="line"><span class="string">        pre_nms_top_n (Dict[int]): number of proposals to keep before applying NMS. It should</span></span><br><span class="line"><span class="string">            contain two fields: training and testing, to allow for different values depending</span></span><br><span class="line"><span class="string">            on training or evaluation</span></span><br><span class="line"><span class="string">        post_nms_top_n (Dict[int]): number of proposals to keep after applying NMS. It should</span></span><br><span class="line"><span class="string">            contain two fields: training and testing, to allow for different values depending</span></span><br><span class="line"><span class="string">            on training or evaluation</span></span><br><span class="line"><span class="string">        nms_thresh (float): NMS threshold used for postprocessing the RPN proposals</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    __annotations__ = &#123;</span><br><span class="line">        <span class="string">'box_coder'</span>: det_utils.BoxCoder,</span><br><span class="line">        <span class="string">'proposal_matcher'</span>: det_utils.Matcher,</span><br><span class="line">        <span class="string">'fg_bg_sampler'</span>: det_utils.BalancedPositiveNegativeSampler,</span><br><span class="line">        <span class="string">'pre_nms_top_n'</span>: Dict[str, int],</span><br><span class="line">        <span class="string">'post_nms_top_n'</span>: Dict[str, int],</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 anchor_generator,</span></span></span><br><span class="line"><span class="function"><span class="params">                 head,</span></span></span><br><span class="line"><span class="function"><span class="params">                 #</span></span></span><br><span class="line"><span class="function"><span class="params">                 fg_iou_thresh, bg_iou_thresh,</span></span></span><br><span class="line"><span class="function"><span class="params">                 batch_size_per_image, positive_fraction,</span></span></span><br><span class="line"><span class="function"><span class="params">                 #</span></span></span><br><span class="line"><span class="function"><span class="params">                 pre_nms_top_n, post_nms_top_n, nms_thresh)</span>:</span></span><br><span class="line">        super(RegionProposalNetwork, self).__init__()</span><br><span class="line">        self.anchor_generator = anchor_generator</span><br><span class="line">        self.head = head</span><br><span class="line">        self.box_coder = det_utils.BoxCoder(weights=(<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># used during training</span></span><br><span class="line">        self.box_similarity = box_ops.box_iou</span><br><span class="line"></span><br><span class="line">        self.proposal_matcher = det_utils.Matcher(</span><br><span class="line">            fg_iou_thresh,</span><br><span class="line">            bg_iou_thresh,</span><br><span class="line">            allow_low_quality_matches=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.fg_bg_sampler = det_utils.BalancedPositiveNegativeSampler(</span><br><span class="line">            batch_size_per_image, positive_fraction</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># used during testing</span></span><br><span class="line">        self._pre_nms_top_n = pre_nms_top_n</span><br><span class="line">        self._post_nms_top_n = post_nms_top_n</span><br><span class="line">        self.nms_thresh = nms_thresh</span><br><span class="line">        self.min_size = <span class="number">1e-3</span></span><br></pre></td></tr></table></figure>
<p>这里无非也就是把两个组件组装了起来，从类的注释可以大致明白各个部分的含义，接下来我门从<code>forward()</code>函数自顶向下地开始，详细地看看它的实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, images, features, targets=None)</span>:</span></span><br><span class="line">    <span class="comment"># type: (ImageList, Dict[str, Tensor], Optional[List[Dict[str, Tensor]]])</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        images (ImageList): images for which we want to compute the predictions</span></span><br><span class="line"><span class="string">        features (List[Tensor]): features computed from the images that are</span></span><br><span class="line"><span class="string">            used for computing the predictions. Each tensor in the list</span></span><br><span class="line"><span class="string">            correspond to different feature levels</span></span><br><span class="line"><span class="string">        targets (List[Dict[Tensor]]): ground-truth boxes present in the image (optional).</span></span><br><span class="line"><span class="string">            If provided, each element in the dict should contain a field `boxes`,</span></span><br><span class="line"><span class="string">            with the locations of the ground-truth boxes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        boxes (List[Tensor]): the predicted boxes from the RPN, one Tensor per</span></span><br><span class="line"><span class="string">            image.</span></span><br><span class="line"><span class="string">        losses (Dict[Tensor]): the losses for the model during training. During</span></span><br><span class="line"><span class="string">            testing, it is an empty dict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># RPN uses all feature maps that are available</span></span><br><span class="line">   	<span class="comment"># 生成anchors 和 各个anchor的objectness logit以及pred_bbox_deltas</span></span><br><span class="line">    features = list(features.values())</span><br><span class="line">    objectness, pred_bbox_deltas = self.head(features)</span><br><span class="line">    anchors = self.anchor_generator(images, features)</span><br><span class="line"></span><br><span class="line">    num_images = len(anchors)</span><br><span class="line">    num_anchors_per_level_shape_tensors = [o[<span class="number">0</span>].shape <span class="keyword">for</span> o <span class="keyword">in</span> objectness]</span><br><span class="line">    num_anchors_per_level = [s[<span class="number">0</span>] * s[<span class="number">1</span>] * s[<span class="number">2</span>] <span class="keyword">for</span> s <span class="keyword">in</span> num_anchors_per_level_shape_tensors]</span><br><span class="line">    objectness, pred_bbox_deltas = \</span><br><span class="line">        concat_box_prediction_layers(objectness, pred_bbox_deltas)</span><br><span class="line">    <span class="comment"># apply pred_bbox_deltas to anchors to obtain the decoded proposals</span></span><br><span class="line">    <span class="comment"># note that we detach the deltas because Faster R-CNN do not backprop through</span></span><br><span class="line">    <span class="comment"># the proposals</span></span><br><span class="line">    proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)</span><br><span class="line">    proposals = proposals.view(num_images, <span class="number">-1</span>, <span class="number">4</span>)</span><br><span class="line">    boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)</span><br><span class="line"></span><br><span class="line">    losses = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> self.training:</span><br><span class="line">        <span class="keyword">assert</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        labels, matched_gt_boxes = self.assign_targets_to_anchors(anchors, targets)</span><br><span class="line">        regression_targets = self.box_coder.encode(matched_gt_boxes, anchors)</span><br><span class="line">        loss_objectness, loss_rpn_box_reg = self.compute_loss(</span><br><span class="line">            objectness, pred_bbox_deltas, labels, regression_targets)</span><br><span class="line">        losses = &#123;</span><br><span class="line">            <span class="string">"loss_objectness"</span>: loss_objectness,</span><br><span class="line">            <span class="string">"loss_rpn_box_reg"</span>: loss_rpn_box_reg,</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">return</span> boxes, losses</span><br></pre></td></tr></table></figure>
<p>使用本节最开始的那个例子，我们用的是resnet50_fpn，所以得到的feature maps是多尺度的，具体而言，会得到5个feature maps，5个尺度的feature tensor的size分别为<code>torch.Size([2, 256, 256, 272]), torch.Size([2, 256, 128, 136]), torch.Size([2, 256, 64, 68]), torch.Size([2, 256, 32, 34]), torch.Size([2, 256, 16, 17])</code>。</p>
<p>插一个题外话，细心的读者可能会思考，明明输入的两张图片是不一样的大小，怎么得到的特征图是一样的？是的，这里有一些细节：</p>
<ul>
<li>首先，分别对每张图片进行等比缩放，缩放后的长宽最小值为800，最大值为1333；</li>
<li>其次，生成一个大小为$(H’,W’)$的全的填充图，$H’和W’$都是32的整倍数，然后把第一步缩放的图片放在填充图的右下角处。</li>
</ul>
<p>具体的代码细节可以参考<code>GeneralizedRCNNTransform</code>类，下图给出了贯穿全文的那个示例对应的图片预处理过程。</p>
<p><img src="preprocess.png" alt="pre_process"></p>
<p>这样一来，一个batch里面所有图片的大小其实都一样，得到的特征图自然也是一样的大小（如果每个batch的size不一样的话又另当别论，那个时候的feature map和当前的肯定大小略有不同，但执行流程是一样的，包括之前提到的anchors生成）。我们继续回到<code>RegionProposalNetwork</code>类的<code>forward()</code>，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多尺度的特征</span></span><br><span class="line">features = list(features.values())</span><br><span class="line"><span class="comment"># 多尺度的objectness和pred_bbox_deltas（那4个关于bbox的参数），是一个list，长度为特征数量</span></span><br><span class="line">objectness, pred_bbox_deltas = self.head(features)</span><br><span class="line"><span class="comment"># 多尺度的anchors（针对每个image的所有feature map的所有cells而言的），是一个list，长度为batch_size</span></span><br><span class="line">anchors = self.anchor_generator(images, features)</span><br><span class="line"></span><br><span class="line">num_images = len(anchors)</span><br><span class="line">num_anchors_per_level_shape_tensors = [o[<span class="number">0</span>].shape <span class="keyword">for</span> o <span class="keyword">in</span> objectness]</span><br><span class="line">num_anchors_per_level = [s[<span class="number">0</span>] * s[<span class="number">1</span>] * s[<span class="number">2</span>] <span class="keyword">for</span> s <span class="keyword">in</span> num_anchors_per_level_shape_tensors]</span><br><span class="line"><span class="comment"># 把所有的objectness和pred_bbox_deltas flatten一下</span></span><br><span class="line"><span class="comment"># 这里得到的 objectness, pred_bbox_deltas第一个维度的大小为 batch_size*anchors_per_image</span></span><br><span class="line">objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)</span><br></pre></td></tr></table></figure>
<p>这样一来，将每张图片多个尺度特征上的所有cells的anchors加起来，一张图片大概有27万个anchors，这也太多了，因此也几乎涵盖了所有可能存在的object的大小了。</p>
<p>随后，我们将anchors和回归得到的pred_bbox_deltas参数做一个计算，得到27万个proposals，也就是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)</span><br><span class="line"><span class="comment"># 得到每张图片的所有proposals，每张图片27万多个</span></span><br><span class="line">proposals = proposals.view(num_images, <span class="number">-1</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># proposals筛选</span></span><br><span class="line">boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)</span><br></pre></td></tr></table></figure>
<p>具体的回归公式如下，其中$t_x,t_y,t_w,t_h$就是rpn 回归层输出的结果，而$x,y,w,h$是gt bbox的中心点水方向坐标、垂直方向坐标、bbox宽、bbox高，$x_a,y_a,w_a,h_a$则是关于anchor的四个参数了。这里我们就相当于已知了$t_x,t_y,t_w,t_h$和$x_a,y_a,w_a,h_a$，想把$x,y,w,h$算出来。</p>
<script type="math/tex; mode=display">
\begin{aligned}
t_{\mathrm{x}} &=\left(x-x_{\mathrm{a}}\right) / w_{\mathrm{a}}, \quad t_{\mathrm{y}}=\left(y-y_{\mathrm{a}}\right) / h_{\mathrm{a}} \\
t_{\mathrm{w}} &=\log \left(w / w_{\mathrm{a}}\right), \quad t_{\mathrm{h}}=\log \left(h / h_{\mathrm{a}}\right)
\end{aligned}</script><p>但正是因为上述过程得到的proposals太多了，我们对RPN这个模块计算loss的时候，不可能把所有的proposals都考虑进来。于是我们进行一个筛选，怎么具体筛选的呢？对于每个尺度的proposals，依照 objectness 置信由大到小度排序（优先提取更可能包含目标的proposals），提取固定数量的proposals。</p>
<ul>
<li>在训练阶段取，对于每个尺度的proposals，如果proposals数量大于2k，则取前2k个，否则就取所有的proposals；对于测试阶段测试阶段，和训练阶段类似，只不过现在只取前1k个）；</li>
<li>然后对所有尺度下选出来的proposas做 NMS操作，生成 boxes （即 NMS 后的 proposal boxes ）；</li>
<li>最后将所有尺度的proposals综合起来，做一个后筛选，如果是训练阶段，最终只保留2k个；如果是测试阶段则最终只保留1K个。</li>
</ul>
<p>如果是训练阶段，还要计算 cls_logits 的损失 loss_objectness，同时计算 bbox_pred 的损失 loss_rpn_box_reg，这里就不详细介绍了。</p>
<p>到这里，rpn的工作就完成了，如果你坚持看到了这里，恭喜你离完全掌握Faster-RCNN不远了，不妨反问一下自己，RPN大概做了哪些事情？</p>
<p>哦，其实就是在不同尺度上，基于不同的anchor大小和aspect ratios，得到很多很多的anchors，并且对每个anchor都输出一个cls logit来表示这个anchor的objectness，并输出4个参数来表示这个anchor box关于真实bbox的修正量，基于这些修正量得到proposals。最后，在不同尺度上都对proposals做一个筛选，筛选方法是按照objectness从大到小排序（越有可能存在物体的proposals在前面），然后做一个NMS筛掉overlap太高的proposals，最后再取前top_N个propoasls。</p>
<p>看完这里，休息一下，吃个饭回来再看后面的部分吧。</p>
<h4 id="2-4-ROI-head"><a href="#2-4-ROI-head" class="headerlink" title="2.4 ROI_head"></a>2.4 ROI_head</h4><p>在RPN得到了所有的boxes之后，我们只大概知道了每个box里面有object，并且认为这个bbox相对而言是比较准确的，我们下一步就是要对这个bbox里面具体是什么东西进行判别，也就是一个分类任务；同时也要对bbox坐标进行refine。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> box_roi_pool <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    box_roi_pool = MultiScaleRoIAlign(</span><br><span class="line">        featmap_names=[<span class="string">'0'</span>, <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>],</span><br><span class="line">        output_size=<span class="number">7</span>,</span><br><span class="line">        sampling_ratio=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> box_head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    resolution = box_roi_pool.output_size[<span class="number">0</span>]</span><br><span class="line">    representation_size = <span class="number">1024</span></span><br><span class="line">    box_head = TwoMLPHead(</span><br><span class="line">        out_channels * resolution ** <span class="number">2</span>,</span><br><span class="line">        representation_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> box_predictor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    representation_size = <span class="number">1024</span></span><br><span class="line">    box_predictor = FastRCNNPredictor(</span><br><span class="line">        representation_size,</span><br><span class="line">        num_classes)</span><br><span class="line"></span><br><span class="line">roi_heads = RoIHeads(</span><br><span class="line">    <span class="comment"># Box</span></span><br><span class="line">    box_roi_pool, box_head, box_predictor,</span><br><span class="line">    box_fg_iou_thresh, box_bg_iou_thresh,</span><br><span class="line">    box_batch_size_per_image, box_positive_fraction,</span><br><span class="line">    bbox_reg_weights,</span><br><span class="line">    box_score_thresh, box_nms_thresh, box_detections_per_img)</span><br></pre></td></tr></table></figure>
<p>第一个问题，想要判别bbox里面的物体，那么我们必定需要提取这个bbox对应的特征，怎么从feature map中快速提取到呢？</p>
<ul>
<li>对于原始 FasterRCNN，只在 backbone 的最后一层 feature_map 提取 box 对应特征；</li>
<li>而加入 FPN 后 backbone 会输出多个特征图，需要计算当前 boxes 对应于哪一个特征。</li>
</ul>
<p>如下图所示：</p>
<p><img src="https://pic4.zhimg.com/80/v2-422396679cf8531c42766841fbd7f927_1440w.jpg" alt="img"></p>
<p>这一部分是通过类<code>MultiScaleRoIAlign</code>来实现的。</p>
<p>首先，对于每个尺度的特征，我们需要直到这个特征相对于输入图片的缩放倍率（这里叫做scale），计算方式如下：</p>
<script type="math/tex; mode=display">
\text { scale }=2^{\text {round }\left(\log _{2}\left(\frac{\text { feature_map_size }}{\text { image_size }}\right)\right)}</script><p>注意一下，这里说的输入图片指的是准备送入backbone之前，缩放了但没有padding的那个大小，对于本文中的那个例子，大小就是(1000,1066)。我个人觉得这个地方有点奇怪，为什么不直接送入padding后的结果，那样显然是2的次幂（因为padding的结果总是32的倍数），不过这里用padding前的也无所谓，后期可以考虑在Github上提出这个issue！代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiScaleRoIAlign</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">   ......</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">infer_scale</span><span class="params">(self, feature, original_size)</span>:</span></span><br><span class="line">        <span class="comment"># type: (Tensor, List[int])</span></span><br><span class="line">        <span class="comment"># assumption: the scale is of the form 2 ** (-k), with k integer</span></span><br><span class="line">        size = feature.shape[<span class="number">-2</span>:]</span><br><span class="line">        possible_scales = torch.jit.annotate(List[float], [])</span><br><span class="line">        <span class="keyword">for</span> s1, s2 <span class="keyword">in</span> zip(size, original_size):</span><br><span class="line">            approx_scale = float(s1) / float(s2)</span><br><span class="line">            scale = <span class="number">2</span> ** float(torch.tensor(approx_scale).log2().round())</span><br><span class="line">            possible_scales.append(scale)</span><br><span class="line">        <span class="keyword">assert</span> possible_scales[<span class="number">0</span>] == possible_scales[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> possible_scales[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup_scales</span><span class="params">(self, features, image_shapes)</span>:</span></span><br><span class="line">        <span class="comment"># type: (List[Tensor], List[Tuple[int, int]])</span></span><br><span class="line">        <span class="keyword">assert</span> len(image_shapes) != <span class="number">0</span></span><br><span class="line">        max_x = <span class="number">0</span></span><br><span class="line">        max_y = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> shape <span class="keyword">in</span> image_shapes:</span><br><span class="line">            max_x = max(shape[<span class="number">0</span>], max_x)</span><br><span class="line">            max_y = max(shape[<span class="number">1</span>], max_y)</span><br><span class="line">        original_input_shape = (max_x, max_y)</span><br><span class="line"></span><br><span class="line">        scales = [self.infer_scale(feat, original_input_shape) <span class="keyword">for</span> feat <span class="keyword">in</span> features]</span><br><span class="line">        <span class="comment"># get the levels in the feature map by leveraging the fact that the network always</span></span><br><span class="line">        <span class="comment"># downsamples by a factor of 2 at each level.</span></span><br><span class="line">        lvl_min = -torch.log2(torch.tensor(scales[<span class="number">0</span>], dtype=torch.float32)).item()</span><br><span class="line">        lvl_max = -torch.log2(torch.tensor(scales[<span class="number">-1</span>], dtype=torch.float32)).item()</span><br><span class="line">        self.scales = scales</span><br><span class="line">        self.map_levels = initLevelMapper(int(lvl_min), int(lvl_max))</span><br></pre></td></tr></table></figure>
<p>对于本文中的例子而言，得到的scales为<code>[1/4, 1/8, 1/16, 1/32]</code>,<code>lvl_min=2, lvl_max=5</code>。</p>
<p>随后，我们的问题就是，对于RPN中得到的每个proposals，我们怎么知道它来自哪个level的feature map呢？这是通过FPN论文[4]中的公式(1)计算得到的，如下：</p>
<script type="math/tex; mode=display">
k=\left\lfloor k_{0}+\log _{2}(\sqrt{w h} / 224)\right\rfloor</script><p>这个公式怎么理解呢？我们知道，对于一个CNN而言，越往后的特征应该对应越high-level的信息，自然应该检测到比较大的bbox，对于FPN而言，如果输入的图片大小为224，那么最后一个特征图可以理解为对224*224大小bbox的特征。因此，上述公式中的$w,h$分别是proposal的宽度和长度，$k_0$则是$wh=224^2$时所对应的那个feature map的level。这一部分的操作是通过<code>initLevelMapper</code>类来实现的，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LevelMapper</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Determine which FPN level each RoI in a set of RoIs should map to based</span></span><br><span class="line"><span class="string">    on the heuristic in the FPN paper.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        k_min (int)</span></span><br><span class="line"><span class="string">        k_max (int)</span></span><br><span class="line"><span class="string">        canonical_scale (int)</span></span><br><span class="line"><span class="string">        canonical_level (int)</span></span><br><span class="line"><span class="string">        eps (float)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k_min, k_max, canonical_scale=<span class="number">224</span>, canonical_level=<span class="number">4</span>, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        <span class="comment"># type: (int, int, int, int, float)</span></span><br><span class="line">        self.k_min = k_min    <span class="comment"># 2</span></span><br><span class="line">        self.k_max = k_max		<span class="comment"># 5</span></span><br><span class="line">        self.s0 = canonical_scale  <span class="comment"># 224</span></span><br><span class="line">        self.lvl0 = canonical_level  <span class="comment"># 4，FPN公式(1)中的k_0</span></span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, boxlists)</span>:</span></span><br><span class="line">        <span class="comment"># type: (List[Tensor])</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Arguments:</span></span><br><span class="line"><span class="string">            boxlists (list[BoxList])</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Compute level ids</span></span><br><span class="line">        s = torch.sqrt(torch.cat([box_area(boxlist) <span class="keyword">for</span> boxlist <span class="keyword">in</span> boxlists]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Eqn.(1) in FPN paper</span></span><br><span class="line">        target_lvls = torch.floor(self.lvl0 + torch.log2(s / self.s0) + torch.tensor(self.eps, dtype=s.dtype))</span><br><span class="line">        target_lvls = torch.clamp(target_lvls, min=self.k_min, max=self.k_max)</span><br><span class="line">        <span class="keyword">return</span> (target_lvls.to(torch.int64) - self.k_min).to(torch.int64)</span><br></pre></td></tr></table></figure>
<p>注意，这里有一个<code>target_lvls = torch.clamp(target_lvls, min=self.k_min, max=self.k_max)</code>的操作，因为有的proposal的大小可能是2*3这么小的，经过上述公式变换可能得到的$k$是一个负值，而我们强行限定最低层的feature map来自于金字塔特征中的第2层；最高层也作类似的限制。也就是如下图所示</p>
<p><img src="https://pic3.zhimg.com/80/v2-e481a19c0b9c33d4f2ec878e3e092f4e_1440w.jpg" alt="img"></p>
<p>至于最后的一个减去<code>lvl_min</code>的操作，是因为输入的金字塔特征是一个OrderDict，key为0对应第2层的特征，所以这里减去<code>lvl_min</code>的意义就是在字典中找到对应的value。</p>
<p>现在我们知道了每个proposal的大小，且其对应的特征来源于哪个level，接下来就是在整个特征图上提取对应于proposal的局部特征，并且对所有的局部做RoI_pooling操作了，直接看代码吧：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiScaleRoIAlign</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">   <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, boxes, image_shapes)</span>:</span></span><br><span class="line">    		<span class="comment"># ...</span></span><br><span class="line">      	</span><br><span class="line">        <span class="comment"># roi pooling之后的结果固定成一样的大小，在这里就是(bs,C,7,7)</span></span><br><span class="line">        result = torch.zeros(</span><br><span class="line">            (num_rois, num_channels,) + self.output_size,</span><br><span class="line">            dtype=dtype,</span><br><span class="line">            device=device,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        tracing_results = []</span><br><span class="line">        <span class="keyword">for</span> level, (per_level_feature, scale) <span class="keyword">in</span> enumerate(zip(x_filtered, scales)):</span><br><span class="line">            idx_in_level = torch.nonzero(levels == level).squeeze(<span class="number">1</span>)</span><br><span class="line">            rois_per_level = rois[idx_in_level]</span><br><span class="line">						<span class="comment"># 提取每个roi对应的局部feature</span></span><br><span class="line">            result_idx_in_level = roi_align(</span><br><span class="line">                per_level_feature, rois_per_level,</span><br><span class="line">                output_size=self.output_size,</span><br><span class="line">                spatial_scale=scale, sampling_ratio=self.sampling_ratio)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> torchvision._is_tracing():</span><br><span class="line">                tracing_results.append(result_idx_in_level.to(dtype))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                result[idx_in_level] = result_idx_in_level</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torchvision._is_tracing():</span><br><span class="line">            result = _onnx_merge_levels(levels, tracing_results)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>从代码中可以看到，会把每个roi对应的局部feature提取出来，也即通过<code>roi_align()</code>函数，这个函数在Pytorch里面就再也往下看不到了，我估计是为了加速计算，其实现肯定是用C/C++写的，这里仅提供了一个Python的接口而已，但是从它传入的几个参数，我们就可以知道它的功能：</p>
<ul>
<li><code>per_level_feature</code>，当前尺度整个batch的特征图；</li>
<li><code>rois_per_level</code>，当前level的所有RoIs；</li>
<li><code>output_size</code>，大小为(7*7)；</li>
<li><code>spatial_scale</code>，当前尺度的特征相对于输入的缩放比例，例如，最低层到最高层的scale依次为1/4、1/8…1/32；</li>
<li><code>sampling_ratio</code>，roi_align 采样率，这个参数好像和Mask-RCNN有关，在这里可以暂时忽略</li>
</ul>
<p>分析到这里，我想你肯定也明白了底层是具体如何实现的吧！举个例子，对于<code>spatial_scale=1/4</code>的情况，如果一个RoI的bbox为$(x_1,y_1,x_2,y_2)$，那么其对应的局部feature的bbox就是$(x_1/4,y_1/4,x_2/4,y_2/4)$了，然后再把这个局部的feature池化到固定的$(7,7)$大小。</p>
<h4 id="2-5-Box-head"><a href="#2-5-Box-head" class="headerlink" title="2.5 Box head"></a>2.5 Box head</h4><p>现在我们得到了所有proposals的粗略的bbox坐标，也知道了每个proposal对应的特征，最后一步是啥？对！对每个proposal分类，原来在RPN里面的cls logit只是判断这个proposal里面有没有object，现在需要在那个基础之上，判断具体是猫、是狗、还是汽车。这在Faster-RCNN里面是通过以下片段定义的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">resolution = box_roi_pool.output_size[<span class="number">0</span>]  <span class="comment"># 7</span></span><br><span class="line">representation_size = <span class="number">1024</span></span><br><span class="line">box_head = TwoMLPHead(</span><br><span class="line">    out_channels * resolution ** <span class="number">2</span>,  <span class="comment"># 256*7*7</span></span><br><span class="line">    representation_size)</span><br></pre></td></tr></table></figure>
<p>而<code>TwoMLPHead</code>的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoMLPHead</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Standard heads for FPN-based models</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        in_channels (int): number of input channels</span></span><br><span class="line"><span class="string">        representation_size (int): size of the intermediate representation</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, representation_size)</span>:</span></span><br><span class="line">        super(TwoMLPHead, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.fc6 = nn.Linear(in_channels, representation_size)</span><br><span class="line">        self.fc7 = nn.Linear(representation_size, representation_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.flatten(start_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.fc6(x))</span><br><span class="line">        x = F.relu(self.fc7(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>其实就是两层全连接，注意，这里得到的是一个中间的embedding表达，维度是1024。在这个基础之上，我们再套两个全连接分别得到各类别的logits，以及regression bbox的refinement。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FastRCNNPredictor</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Standard classification + bounding box regression layers</span></span><br><span class="line"><span class="string">    for Fast R-CNN.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        in_channels (int): number of input channels</span></span><br><span class="line"><span class="string">        num_classes (int): number of output classes (including background)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, num_classes)</span>:</span></span><br><span class="line">        super(FastRCNNPredictor, self).__init__()</span><br><span class="line">        self.cls_score = nn.Linear(in_channels, num_classes)</span><br><span class="line">        self.bbox_pred = nn.Linear(in_channels, num_classes * <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> x.dim() == <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">assert</span> list(x.shape[<span class="number">2</span>:]) == [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        x = x.flatten(start_dim=<span class="number">1</span>)</span><br><span class="line">        scores = self.cls_score(x)</span><br><span class="line">        bbox_deltas = self.bbox_pred(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> scores, bbox_deltas</span><br></pre></td></tr></table></figure>
<p>Box head的整个流程可以用下图概括：</p>
<p><img src="https://pic2.zhimg.com/80/v2-f4fdabdefe468fd856f37e4557f3dded_1440w.jpg" alt="img"></p>
<p>显然 cls_score 后接 softmax 即为类别概率，可以确定 box 的类别；在确定类别后，在 bbox_pred 中对应类别的 <img src="https://www.zhihu.com/equation?tex=%28dx%2Cdy%2Cdw%2Cdh%29" alt="[公式]"> 4个值即为第二次 bounding box regression 需要的4个偏移值，原来在RPN中的4个偏移值是相对anchors而言的，而现在的偏移量则是相对proposals而言的，具体计算方法一摸一样。</p>
<h4 id="2-6-最后的筛选"><a href="#2-6-最后的筛选" class="headerlink" title="2.6 最后的筛选"></a>2.6 最后的筛选</h4><p>经过上述的RPN和RoI pooling之后，proposals的数量还是很多，我们要做一些筛选，整体规则如下：</p>
<ul>
<li>首先，筛除掉cls_socre太小的bbox，阈值为默认为0.05；</li>
<li>其次，筛除掉面积很小很小的bbox，阈值为0.01;</li>
<li>随后，对于输出的每个类别，作一次NMS，NMS的阈值为0.5；</li>
<li>在做完NMS后，对于每张图片，如果所有类别的bbox总和数量还是很大，则取前<code>detections_per_img</code>个，这个值默认为100；</li>
</ul>
<p>这样一来，每张图片最多有100个bbox，这个数量就比较合理了。</p>
<h4 id="2-7-transform逆变换"><a href="#2-7-transform逆变换" class="headerlink" title="2.7 transform逆变换"></a>2.7 transform逆变换</h4><p>注意到，我们上述的所有过程都是在resize之后的图片（例如，800*1000）求出来的，bbox的大小也是自然也是相对于800*100而言的，但原始的输入图片大小为400*500，那么最后我们必然要把bbox缩放回去，这个和本文2.2小节中的transform对应，只不过是它的逆变换而言，具体的代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GeneralizedRCNNTransform</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="comment"># ...</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">postprocess</span><span class="params">(self, result, image_shapes, original_image_sizes)</span>:</span></span><br><span class="line">      <span class="comment"># type: (List[Dict[str, Tensor]], List[Tuple[int, int]], List[Tuple[int, int]])</span></span><br><span class="line">      <span class="keyword">if</span> self.training:</span><br><span class="line">          <span class="keyword">return</span> result</span><br><span class="line">      <span class="keyword">for</span> i, (pred, im_s, o_im_s) <span class="keyword">in</span> enumerate(zip(result, image_shapes, original_image_sizes)):</span><br><span class="line">          boxes = pred[<span class="string">"boxes"</span>]</span><br><span class="line">          boxes = resize_boxes(boxes, im_s, o_im_s)</span><br><span class="line">          result[i][<span class="string">"boxes"</span>] = boxes</span><br><span class="line"></span><br><span class="line">          <span class="comment"># ...</span></span><br><span class="line">       <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h3 id="3-最后的最后"><a href="#3-最后的最后" class="headerlink" title="3. 最后的最后"></a>3. 最后的最后</h3><p>分析到这里，Faster-RCNN基本所有的流程都介绍清楚了，唯独缺少的就是没讲怎么训练的，其实也就是训练RPN和最后的box head，这些内容在<a href="https://zhuanlan.zhihu.com/p/145842317" target="_blank" rel="noopener">[1]</a>中有介绍，有兴趣可以参阅一下。本文cover的主要是inference阶段。</p>
<p>最后，我们可以用下图来总结一下Faster-RCNN的流程：</p>
<p><img src="https://pic2.zhimg.com/80/v2-d85d110b694f1d863f565c082c326e2d_1440w.jpg" alt="img"></p>
<h3 id="4-Demo"><a href="#4-Demo" class="headerlink" title="4. Demo"></a>4. Demo</h3><blockquote>
<p>Things I cannot touch, I don’t  believe.             — by Me</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models.detection.faster_rcnn <span class="keyword">import</span> fasterrcnn_resnet50_fpn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_color</span><span class="params">()</span>:</span></span><br><span class="line">    b = random.randint(<span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    g = random.randint(<span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    r = random.randint(<span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (b, g, r)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">coco_names = &#123;<span class="string">'0'</span>: <span class="string">'background'</span>, <span class="string">'1'</span>: <span class="string">'person'</span>, <span class="string">'2'</span>: <span class="string">'bicycle'</span>, <span class="string">'3'</span>: <span class="string">'car'</span>, <span class="string">'4'</span>: <span class="string">'motorcycle'</span>, <span class="string">'5'</span>: <span class="string">'airplane'</span>,</span><br><span class="line">              <span class="string">'6'</span>: <span class="string">'bus'</span>, <span class="string">'7'</span>: <span class="string">'train'</span>, <span class="string">'8'</span>: <span class="string">'truck'</span>, <span class="string">'9'</span>: <span class="string">'boat'</span>, <span class="string">'10'</span>: <span class="string">'traffic light'</span>, <span class="string">'11'</span>: <span class="string">'fire hydrant'</span>,</span><br><span class="line">              <span class="string">'13'</span>: <span class="string">'stop sign'</span>, <span class="string">'14'</span>: <span class="string">'parking meter'</span>, <span class="string">'15'</span>: <span class="string">'bench'</span>, <span class="string">'16'</span>: <span class="string">'bird'</span>, <span class="string">'17'</span>: <span class="string">'cat'</span>, <span class="string">'18'</span>: <span class="string">'dog'</span>,</span><br><span class="line">              <span class="string">'19'</span>: <span class="string">'horse'</span>, <span class="string">'20'</span>: <span class="string">'sheep'</span>, <span class="string">'21'</span>: <span class="string">'cow'</span>, <span class="string">'22'</span>: <span class="string">'elephant'</span>, <span class="string">'23'</span>: <span class="string">'bear'</span>, <span class="string">'24'</span>: <span class="string">'zebra'</span>, <span class="string">'25'</span>: <span class="string">'giraffe'</span>,</span><br><span class="line">              <span class="string">'27'</span>: <span class="string">'backpack'</span>, <span class="string">'28'</span>: <span class="string">'umbrella'</span>, <span class="string">'31'</span>: <span class="string">'handbag'</span>, <span class="string">'32'</span>: <span class="string">'tie'</span>, <span class="string">'33'</span>: <span class="string">'suitcase'</span>, <span class="string">'34'</span>: <span class="string">'frisbee'</span>,</span><br><span class="line">              <span class="string">'35'</span>: <span class="string">'skis'</span>, <span class="string">'36'</span>: <span class="string">'snowboard'</span>, <span class="string">'37'</span>: <span class="string">'sports ball'</span>, <span class="string">'38'</span>: <span class="string">'kite'</span>, <span class="string">'39'</span>: <span class="string">'baseball bat'</span>,</span><br><span class="line">              <span class="string">'40'</span>: <span class="string">'baseball glove'</span>, <span class="string">'41'</span>: <span class="string">'skateboard'</span>, <span class="string">'42'</span>: <span class="string">'surfboard'</span>, <span class="string">'43'</span>: <span class="string">'tennis racket'</span>, <span class="string">'44'</span>: <span class="string">'bottle'</span>,</span><br><span class="line">              <span class="string">'46'</span>: <span class="string">'wine glass'</span>, <span class="string">'47'</span>: <span class="string">'cup'</span>, <span class="string">'48'</span>: <span class="string">'fork'</span>, <span class="string">'49'</span>: <span class="string">'knife'</span>, <span class="string">'50'</span>: <span class="string">'spoon'</span>, <span class="string">'51'</span>: <span class="string">'bowl'</span>, <span class="string">'52'</span>: <span class="string">'banana'</span>,</span><br><span class="line">              <span class="string">'53'</span>: <span class="string">'apple'</span>, <span class="string">'54'</span>: <span class="string">'sandwich'</span>, <span class="string">'55'</span>: <span class="string">'orange'</span>, <span class="string">'56'</span>: <span class="string">'broccoli'</span>, <span class="string">'57'</span>: <span class="string">'carrot'</span>, <span class="string">'58'</span>: <span class="string">'hot dog'</span>,</span><br><span class="line">              <span class="string">'59'</span>: <span class="string">'pizza'</span>, <span class="string">'60'</span>: <span class="string">'donut'</span>, <span class="string">'61'</span>: <span class="string">'cake'</span>, <span class="string">'62'</span>: <span class="string">'chair'</span>, <span class="string">'63'</span>: <span class="string">'couch'</span>, <span class="string">'64'</span>: <span class="string">'potted plant'</span>,</span><br><span class="line">              <span class="string">'65'</span>: <span class="string">'bed'</span>, <span class="string">'67'</span>: <span class="string">'dining table'</span>, <span class="string">'70'</span>: <span class="string">'toilet'</span>, <span class="string">'72'</span>: <span class="string">'tv'</span>, <span class="string">'73'</span>: <span class="string">'laptop'</span>, <span class="string">'74'</span>: <span class="string">'mouse'</span>,</span><br><span class="line">              <span class="string">'75'</span>: <span class="string">'remote'</span>, <span class="string">'76'</span>: <span class="string">'keyboard'</span>, <span class="string">'77'</span>: <span class="string">'cell phone'</span>, <span class="string">'78'</span>: <span class="string">'microwave'</span>, <span class="string">'79'</span>: <span class="string">'oven'</span>, <span class="string">'80'</span>: <span class="string">'toaster'</span>,</span><br><span class="line">              <span class="string">'81'</span>: <span class="string">'sink'</span>, <span class="string">'82'</span>: <span class="string">'refrigerator'</span>, <span class="string">'84'</span>: <span class="string">'book'</span>, <span class="string">'85'</span>: <span class="string">'clock'</span>, <span class="string">'86'</span>: <span class="string">'vase'</span>, <span class="string">'87'</span>: <span class="string">'scissors'</span>,</span><br><span class="line">              <span class="string">'88'</span>: <span class="string">'teddybear'</span>, <span class="string">'89'</span>: <span class="string">'hair drier'</span>, <span class="string">'90'</span>: <span class="string">'toothbrush'</span>&#125;</span><br><span class="line"></span><br><span class="line">SCORE_THRESHOLD = <span class="number">0.6</span></span><br><span class="line"></span><br><span class="line">model = fasterrcnn_resnet50_fpn(pretrained=<span class="literal">True</span>)</span><br><span class="line">model.eval()</span><br><span class="line">img1 = cv2.imread(<span class="string">"img1.jpeg"</span>, cv2.COLOR_BGR2RGB) / <span class="number">255.0</span></span><br><span class="line">img2 = cv2.imread(<span class="string">"img2.jpeg"</span>, cv2.COLOR_BGR2RGB) / <span class="number">255.0</span></span><br><span class="line">x = [</span><br><span class="line">    torch.from_numpy(np.transpose(img1, axes=[<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])).to(torch.float32),</span><br><span class="line">    torch.from_numpy(np.transpose(img1, axes=[<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])).to(torch.float32)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">out = model(x)</span><br><span class="line"></span><br><span class="line">boxes = out[<span class="number">0</span>][<span class="string">'boxes'</span>]</span><br><span class="line">labels = out[<span class="number">0</span>][<span class="string">'labels'</span>]</span><br><span class="line">scores = out[<span class="number">0</span>][<span class="string">'scores'</span>]</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> range(boxes.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> scores[idx] &gt;= SCORE_THRESHOLD:</span><br><span class="line">        x1, y1, x2, y2 = boxes[idx][<span class="number">0</span>], boxes[idx][<span class="number">1</span>], boxes[idx][<span class="number">2</span>], boxes[idx][<span class="number">3</span>]</span><br><span class="line">        name = coco_names.get(str(labels[idx].item()))</span><br><span class="line">        <span class="comment"># cv2.rectangle(img,(x1,y1),(x2,y2),colors[labels[idx].item()],thickness=2)</span></span><br><span class="line">        cv2.rectangle(img1, (x1, y1), (x2, y2), random_color(), thickness=<span class="number">2</span>)</span><br><span class="line">        cv2.putText(img1, text=name, org=(x1, y1 + <span class="number">10</span>), fontFace=cv2.FONT_HERSHEY_SIMPLEX,</span><br><span class="line">                    fontScale=<span class="number">0.5</span>, thickness=<span class="number">1</span>, lineType=cv2.LINE_AA, color=(<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>))</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'img1'</span>, img1)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.imwrite(<span class="string">'img_out'</span>,img1)</span><br></pre></td></tr></table></figure>
<p>随手拍了一张图片，下面分别是score_threshold为0.2时的输出、score_threshold为0.5时的输出。</p>
<p><img src="img2_out_th02.jpeg" alt="out_2_th0.2"></p>
<p><img src="img2_out_th05.jpeg" alt="out_2_th0.5"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>【1】<a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">一文读懂Faster-RCNN</a></p>
<p>【2】<a href="https://zhuanlan.zhihu.com/p/145842317" target="_blank" rel="noopener">捋一捋pytorch官方FasterRCNN代码</a>，感谢作者的分享，本文大量图片来源于此。</p>
<p>【3】<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p>
<p>【4】<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">Feature Pyramid Networks for Object Detection</a></p>


                <hr>

                

                <ul class="pager">
                    
                    
                        <li class="next">
                            <a href="/2020/10/02/tmux初体验/" data-toggle="tooltip" data-placement="top" title="tmux初体验">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                

                
				
				
				<!-- 来必力City版安装代码 -->
				<div id="lv-container" data-id="city" data-uid="MTAyMC80NzM2OC8yMzg2OA==" >
					<script type="text/javascript">
				   (function(d, s) {
					   var j, e = d.getElementsByTagName(s)[0];

					   if (typeof LivereTower === 'function') { return; }

					   j = d.createElement(s);
					   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
					   j.async = true;

					   e.parentNode.insertBefore(j, e);
				   })(document, 'script');
					</script>
				<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
				</div>
				<!-- City版安装代码已完成 -->
				

            </div>
    <!-- Side Catalog Container -->
        

    <!-- Sidebar Container -->

            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Object Detection" title="Object Detection">Object Detection</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="http://huangxuan.me" target="_blank">Hux Blog</a></li>
                    
                        <li><a href="#" target="_blank">Foo</a></li>
                    
                        <li><a href="#" target="_blank">Bar</a></li>
                    
                </ul>
                
            </div>

        </div>
    </div>
</article>







<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/liu-zhi-an-98">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="http://weibo.com/u/1952886932">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                
                    <li>
                        <a target="_blank"  href="https://github.com/LiUzHiAn">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; LiuZhian&#39;s Blog 2020 
                    <br>
                    Theme by <a href="http://huangxuan.me" target="_blank" rel="noopener">Hux</a> 
                    <span style="display: inline-block; margin: 0 5px;">
                        <i class="fa fa-heart"></i>
                    </span> 
                    Ported by <a href="http://blog.kaijun.rocks" target="_blank" rel="noopener">Kaijun</a> | 
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="91px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=kaijun&repo=hexo-theme-huxblog&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>



<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://liuzhian.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-150994646-1';
    var _gaDomain = '';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Side Catalog -->





<!-- Image to hack wechat -->
<img src="https://liuzhian.github.io/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work --><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>

</html>
